{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44475ec",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "#Modelo regressao com infos:\n",
    "\n",
    "# ============ IMPORTA√á√ïES COMPLETAS ============\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from shapely.geometry import box, Polygon\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score, accuracy_score, precision_score, \\\n",
    "    recall_score, f1_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.inspection import permutation_importance\n",
    "from scipy import ndimage\n",
    "import seaborn as sns\n",
    "import time\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# ============ CONFIGURA√á√ÉO ============\n",
    "class Config:\n",
    "    \"\"\"Configura√ß√µes para regress√£o preditiva temporal\"\"\"\n",
    "    DATA_PATH = Path(\"C:/Users/Rikar/Downloads/Projeto/csv/csv/\")\n",
    "    LAT_COL, LON_COL = \"LATITUDE\", \"LONGITUDE\"\n",
    "    CELL_SIZE = 0.01\n",
    "\n",
    "    # üî• DEFINI√á√ÉO EXPL√çCITA TEMPORAL\n",
    "    ANOS_DISPONIVEIS = [2019, 2020, 2021, 2022]\n",
    "    ANOS_TREINO = [2019, 2020, 2021]\n",
    "    ANO_TESTE = 2022\n",
    "\n",
    "    # Garantir que n√£o h√° overlap\n",
    "    assert ANO_TESTE not in ANOS_TREINO, \"‚ùå Ano de teste n√£o pode estar no treino!\"\n",
    "    assert ANO_TESTE in ANOS_DISPONIVEIS, \"‚ùå Ano de teste deve estar dispon√≠vel!\"\n",
    "\n",
    "    # Par√¢metros XGBoost\n",
    "    XGB_PARAMS = {\n",
    "        'n_estimators': 300,\n",
    "        'max_depth': 10,\n",
    "        'learning_rate': 0.05,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'reg_alpha': 0.1,\n",
    "        'reg_lambda': 0.1,\n",
    "        'random_state': 42,\n",
    "        'n_jobs': -1\n",
    "    }\n",
    "\n",
    "    HOTSPOT_PERCENTIL = 85\n",
    "    THRESHOLD_MINIMO = 5  # M√≠nimo de crimes para ser considerado hotspot\n",
    "\n",
    "\n",
    "# ============ CARREGAMENTO DE DADOS ============\n",
    "class DataLoader:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def carregar_ano(self, ano):\n",
    "        \"\"\"Carrega dados de um ano espec√≠fico\"\"\"\n",
    "        try:\n",
    "            arquivo = self.config.DATA_PATH / f\"SPSafe_{ano}.csv\"\n",
    "            if not arquivo.exists():\n",
    "                print(f\"   ‚ùå Arquivo {arquivo} n√£o encontrado\")\n",
    "                return None\n",
    "\n",
    "            df = pd.read_csv(\n",
    "                arquivo,\n",
    "                sep=None,\n",
    "                engine=\"python\",\n",
    "                encoding_errors=\"ignore\",\n",
    "                usecols=[self.config.LAT_COL, self.config.LON_COL, \"NATUREZA_APURADA\",\n",
    "                         \"DATA_OCORRENCIA\", \"HORA_OCORRENCIA\", \"PERIODO_OCORRENCIA\",\n",
    "                         \"TIPO_LOCAL\", \"IDADE_PESSOA\"],\n",
    "                dtype={\n",
    "                    'NATUREZA_APURADA': 'category',\n",
    "                    'PERIODO_OCORRENCIA': 'category',\n",
    "                    'TIPO_LOCAL': 'category'\n",
    "                }\n",
    "            )\n",
    "            df['ANO'] = ano\n",
    "            print(f\"   ‚úÖ {ano}: {len(df):,} registros\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao carregar {ano}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ============ PROCESSAMENTO ESPACIAL ============\n",
    "class SpatialProcessor:\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def processar_dados(self, df, nome_conjunto):\n",
    "        \"\"\"Processa e limpa os dados\"\"\"\n",
    "        print(f\"üîß Processando {nome_conjunto} ({len(df):,} registros)...\")\n",
    "\n",
    "        # Limpeza b√°sica\n",
    "        initial_count = len(df)\n",
    "        df = df.dropna(subset=[self.config.LAT_COL, self.config.LON_COL])\n",
    "        df = df[(df[self.config.LAT_COL].between(-90, 90)) &\n",
    "                (df[self.config.LON_COL].between(-180, 180))]\n",
    "\n",
    "        print(f\"   ‚úÖ {len(df):,} coordenadas v√°lidas (de {initial_count:,} inicial)\")\n",
    "\n",
    "        # Features temporais\n",
    "        df['DATA_OCORRENCIA'] = pd.to_datetime(df['DATA_OCORRENCIA'], errors='coerce')\n",
    "        df['MES'] = df['DATA_OCORRENCIA'].dt.month\n",
    "        df['DIA_SEMANA'] = df['DATA_OCORRENCIA'].dt.dayofweek\n",
    "        df['FIM_DE_SEMANA'] = df['DIA_SEMANA'].isin([5, 6]).astype(int)\n",
    "\n",
    "        # Criar geodataframe\n",
    "        gdf = gpd.GeoDataFrame(\n",
    "            df,\n",
    "            geometry=gpd.points_from_xy(df[self.config.LON_COL], df[self.config.LAT_COL]),\n",
    "            crs=\"EPSG:4326\"\n",
    "        )\n",
    "        return gdf\n",
    "\n",
    "    def carregar_limites_sp(self):\n",
    "        \"\"\"Carrega os limites de S√£o Paulo\"\"\"\n",
    "        print(\"üèûÔ∏è Carregando limites do Estado de S√£o Paulo...\")\n",
    "        try:\n",
    "            fonte_alternativa = \"https://raw.githubusercontent.com/codeforamerica/click_that_hood/master/public/data/brazil-states.geojson\"\n",
    "            estados = gpd.read_file(fonte_alternativa)\n",
    "            sao_paulo = estados[estados['name'] == 'S√£o Paulo']\n",
    "            if not sao_paulo.empty:\n",
    "                print(\"‚úÖ Limites carregados da fonte alternativa!\")\n",
    "                return sao_paulo.reset_index(drop=True)\n",
    "            else:\n",
    "                raise ValueError(\"S√£o Paulo n√£o encontrado\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro: {e}\")\n",
    "            # Fallback: bounding box de SP\n",
    "            sp_bbox = Polygon([\n",
    "                (-53.11, -25.33), (-44.15, -25.33),\n",
    "                (-44.15, -19.77), (-53.11, -19.77),\n",
    "                (-53.11, -25.33)\n",
    "            ])\n",
    "            sao_paulo_fallback = gpd.GeoDataFrame({\n",
    "                'nome': ['S√£o Paulo'], 'sigla': ['SP'], 'geometry': [sp_bbox]\n",
    "            }, crs=\"EPSG:4326\")\n",
    "            print(\"‚úÖ Usando bounding box como fallback\")\n",
    "            return sao_paulo_fallback\n",
    "\n",
    "    def criar_grid_e_features(self, gdf, sao_paulo, nome_conjunto):\n",
    "        \"\"\"Cria grid espacial e calcula features\"\"\"\n",
    "        print(f\"üì¶ Criando grid e features para {nome_conjunto}...\")\n",
    "\n",
    "        # Filtrar dados para SP\n",
    "        gdf_sp = gpd.sjoin(gdf, sao_paulo, how=\"inner\", predicate=\"within\")\n",
    "        print(f\"   ‚úÖ {len(gdf_sp):,} ocorr√™ncias dentro de S√£o Paulo\")\n",
    "\n",
    "        # Criar grid\n",
    "        xmin, ymin, xmax, ymax = sao_paulo.total_bounds\n",
    "        xmin = np.floor(xmin / self.config.CELL_SIZE) * self.config.CELL_SIZE\n",
    "        ymin = np.floor(ymin / self.config.CELL_SIZE) * self.config.CELL_SIZE\n",
    "        xmax = np.ceil(xmax / self.config.CELL_SIZE) * self.config.CELL_SIZE\n",
    "        ymax = np.ceil(ymax / self.config.CELL_SIZE) * self.config.CELL_SIZE\n",
    "\n",
    "        x_coords = np.arange(xmin, xmax, self.config.CELL_SIZE)\n",
    "        y_coords = np.arange(ymin, ymax, self.config.CELL_SIZE)\n",
    "\n",
    "        # Criar c√©lulas do grid\n",
    "        polygons = []\n",
    "        for x in x_coords:\n",
    "            for y in y_coords:\n",
    "                polygons.append(box(x, y, x + self.config.CELL_SIZE, y + self.config.CELL_SIZE))\n",
    "\n",
    "        grid_sp = gpd.GeoDataFrame({'geometry': polygons}, crs=\"EPSG:4326\")\n",
    "        grid_sp[\"cell_id\"] = grid_sp.index\n",
    "\n",
    "        # Filtrar c√©lulas dentro de SP\n",
    "        grid_sp_filtered = gpd.sjoin(grid_sp, sao_paulo, how=\"inner\", predicate=\"intersects\")\n",
    "\n",
    "        # Spatial join seguro\n",
    "        gdf_sp_clean = gdf_sp.reset_index(drop=True).copy()\n",
    "        grid_sp_clean = grid_sp_filtered.reset_index(drop=True).copy()\n",
    "\n",
    "        for df_temp in [gdf_sp_clean, grid_sp_clean]:\n",
    "            if 'index_right' in df_temp.columns:\n",
    "                df_temp.drop(columns=['index_right'], inplace=True)\n",
    "            if 'index_left' in df_temp.columns:\n",
    "                df_temp.drop(columns=['index_left'], inplace=True)\n",
    "\n",
    "        joined_sp = gpd.sjoin(\n",
    "            gdf_sp_clean,\n",
    "            grid_sp_clean[['geometry', 'cell_id']],\n",
    "            how=\"left\",\n",
    "            predicate=\"within\",\n",
    "            lsuffix='_point',\n",
    "            rsuffix='_grid'\n",
    "        )\n",
    "\n",
    "        # Contagem por c√©lula\n",
    "        cell_counts_sp = joined_sp[\"cell_id\"].value_counts().rename_axis(\"cell_id\").reset_index(name=\"count\")\n",
    "        grid_final = grid_sp_clean.merge(cell_counts_sp, on=\"cell_id\", how=\"left\").fillna({\"count\": 0})\n",
    "\n",
    "        # Converter coordenadas\n",
    "        grid_final_proj = grid_final.to_crs('EPSG:3857')\n",
    "        grid_final_proj['centroid'] = grid_final_proj.geometry.centroid\n",
    "        grid_final['x'] = grid_final_proj.centroid.x\n",
    "        grid_final['y'] = grid_final_proj.centroid.y\n",
    "\n",
    "        # Features avan√ßadas\n",
    "        features_df = self._extrair_features_avancadas(joined_sp, grid_final)\n",
    "        grid_final = grid_final.merge(features_df, on='cell_id', how='left')\n",
    "\n",
    "        # Preencher valores faltantes\n",
    "        feature_cols = ['periodos_diversidade', 'naturezas_diversidade', 'locais_diversidade',\n",
    "                        'idade_media', 'idade_std', 'concentracao_temporal', 'densidade_vizinhanca']\n",
    "        for col in feature_cols:\n",
    "            if col in grid_final.columns:\n",
    "                grid_final[col] = grid_final[col].fillna(0)\n",
    "            else:\n",
    "                grid_final[col] = 0\n",
    "\n",
    "        print(f\"   ‚úÖ {len(grid_final):,} c√©lulas processadas\")\n",
    "        return grid_final\n",
    "\n",
    "    def _extrair_features_avancadas(self, joined_sp, grid_final):\n",
    "        \"\"\"Extrai features avan√ßadas dos dados\"\"\"\n",
    "        if len(joined_sp) == 0:\n",
    "            return pd.DataFrame({\n",
    "                'cell_id': grid_final['cell_id'],\n",
    "                'periodos_diversidade': 0, 'naturezas_diversidade': 0,\n",
    "                'locais_diversidade': 0, 'idade_media': 0, 'idade_std': 0,\n",
    "                'concentracao_temporal': 0, 'densidade_vizinhanca': 0\n",
    "            })\n",
    "\n",
    "        try:\n",
    "            grouped = joined_sp.groupby('cell_id')\n",
    "\n",
    "            # Diversidade de caracter√≠sticas\n",
    "            diversidade_periodos = grouped['PERIODO_OCORRENCIA'].nunique().rename('periodos_diversidade').reset_index()\n",
    "            diversidade_naturezas = grouped['NATUREZA_APURADA'].nunique().rename('naturezas_diversidade').reset_index()\n",
    "            diversidade_locais = grouped['TIPO_LOCAL'].nunique().rename('locais_diversidade').reset_index()\n",
    "\n",
    "            # Estat√≠sticas de idade\n",
    "            if 'IDADE_PESSOA' in joined_sp.columns:\n",
    "                estatisticas_idade = grouped['IDADE_PESSOA'].agg(['mean', 'std']).reset_index()\n",
    "                estatisticas_idade = estatisticas_idade.rename(columns={'mean': 'idade_media', 'std': 'idade_std'})\n",
    "            else:\n",
    "                estatisticas_idade = pd.DataFrame({\n",
    "                    'cell_id': grid_final['cell_id'],\n",
    "                    'idade_media': 0, 'idade_std': 0\n",
    "                })\n",
    "\n",
    "            # Concentra√ß√£o temporal\n",
    "            concentracao_temporal_df = self._calcular_concentracao_temporal(grouped, grid_final)\n",
    "\n",
    "            # Densidade espacial\n",
    "            densidade_df = self._calcular_densidade_espacial(grid_final)\n",
    "\n",
    "            # Consolidar features\n",
    "            features_df = diversidade_periodos.copy()\n",
    "            features_df = features_df.merge(diversidade_naturezas, on='cell_id', how='left')\n",
    "            features_df = features_df.merge(diversidade_locais, on='cell_id', how='left')\n",
    "            features_df = features_df.merge(estatisticas_idade, on='cell_id', how='left')\n",
    "            features_df = features_df.merge(concentracao_temporal_df, on='cell_id', how='left')\n",
    "            features_df = features_df.merge(densidade_df, on='cell_id', how='left')\n",
    "\n",
    "            return features_df.fillna(0)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Erro ao extrair features: {e}\")\n",
    "            return pd.DataFrame({\n",
    "                'cell_id': grid_final['cell_id'],\n",
    "                'periodos_diversidade': 0, 'naturezas_diversidade': 0,\n",
    "                'locais_diversidade': 0, 'idade_media': 0, 'idade_std': 0,\n",
    "                'concentracao_temporal': 0, 'densidade_vizinhanca': 0\n",
    "            })\n",
    "\n",
    "    def _calcular_concentracao_temporal(self, grouped, grid_final):\n",
    "        \"\"\"Calcula concentra√ß√£o temporal dos crimes\"\"\"\n",
    "\n",
    "        def calcular_concentracao(group_df):\n",
    "            if len(group_df) == 0 or 'MES' not in group_df.columns:\n",
    "                return 0\n",
    "            mes_counts = group_df['MES'].value_counts()\n",
    "            return mes_counts.max() / len(group_df)\n",
    "\n",
    "        cells_com_dados = grid_final[grid_final['count'] > 0]['cell_id']\n",
    "        concentracao_data = []\n",
    "\n",
    "        for cell_id in cells_com_dados:\n",
    "            try:\n",
    "                group_data = grouped.get_group(cell_id)\n",
    "                concentracao = calcular_concentracao(group_data)\n",
    "                concentracao_data.append({'cell_id': cell_id, 'concentracao_temporal': concentracao})\n",
    "            except KeyError:\n",
    "                concentracao_data.append({'cell_id': cell_id, 'concentracao_temporal': 0})\n",
    "\n",
    "        return pd.DataFrame(concentracao_data)\n",
    "\n",
    "    def _calcular_densidade_espacial(self, grid_final):\n",
    "        \"\"\"Calcula densidade espacial usando suaviza√ß√£o gaussiana\"\"\"\n",
    "        try:\n",
    "            x_coords_density = np.linspace(grid_final['x'].min(), grid_final['x'].max(), 50)\n",
    "            y_coords_density = np.linspace(grid_final['y'].min(), grid_final['y'].max(), 50)\n",
    "\n",
    "            density_matrix = np.zeros((len(y_coords_density), len(x_coords_density)))\n",
    "\n",
    "            for idx, row in grid_final.iterrows():\n",
    "                x_idx = np.abs(x_coords_density - row['x']).argmin()\n",
    "                y_idx = np.abs(y_coords_density - row['y']).argmin()\n",
    "                if x_idx < len(x_coords_density) and y_idx < len(y_coords_density):\n",
    "                    density_matrix[y_idx, x_idx] = row['count']\n",
    "\n",
    "            density_smoothed = ndimage.gaussian_filter(density_matrix, sigma=1)\n",
    "\n",
    "            densidade_data = []\n",
    "            for idx, row in grid_final.iterrows():\n",
    "                x_idx = np.abs(x_coords_density - row['x']).argmin()\n",
    "                y_idx = np.abs(y_coords_density - row['y']).argmin()\n",
    "                if x_idx < len(x_coords_density) and y_idx < len(y_coords_density):\n",
    "                    densidade = float(density_smoothed[y_idx, x_idx])\n",
    "                else:\n",
    "                    densidade = 0.0\n",
    "                densidade_data.append({'cell_id': row['cell_id'], 'densidade_vizinhanca': densidade})\n",
    "\n",
    "            return pd.DataFrame(densidade_data)\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ö†Ô∏è  Erro no c√°lculo de densidade: {e}\")\n",
    "            return pd.DataFrame({'cell_id': grid_final['cell_id'], 'densidade_vizinhanca': 0.0})\n",
    "\n",
    "\n",
    "# ============ PROCESSADOR TEMPORAL SEGURO ============\n",
    "class SafeTemporalProcessor:\n",
    "    \"\"\"Processamento que GARANTE separa√ß√£o temporal\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.scaler = None\n",
    "        self.features_para_usar = None\n",
    "\n",
    "    def processar_conjunto_treino(self, anos_treino):\n",
    "        \"\"\"Processa APENAS dados de treino - sem informa√ß√£o do teste\"\"\"\n",
    "        print(\"üìä Processando conjunto de TREINO...\")\n",
    "\n",
    "        grids_treino = []\n",
    "        for ano in anos_treino:\n",
    "            grid = self._processar_ano_individual(ano, f\"Treino {ano}\")\n",
    "            if grid is not None:\n",
    "                grids_treino.append(grid)\n",
    "\n",
    "        if not grids_treino:\n",
    "            raise ValueError(\"‚ùå Nenhum dado de treino processado!\")\n",
    "\n",
    "        # Combinar treino\n",
    "        grid_treino_combinado = pd.concat(grids_treino, ignore_index=True)\n",
    "\n",
    "        # Definir features APENAS com base no treino\n",
    "        self.features_para_usar = self._definir_features(grid_treino_combinado)\n",
    "\n",
    "        # Treinar scaler APENAS com dados de treino\n",
    "        X_treino = grid_treino_combinado[self.features_para_usar]\n",
    "        self.scaler = StandardScaler()\n",
    "        X_treino_scaled = self.scaler.fit_transform(X_treino)\n",
    "\n",
    "        y_treino = grid_treino_combinado['count']\n",
    "\n",
    "        print(f\"   ‚úÖ Treino: {len(grid_treino_combinado):,} c√©lulas\")\n",
    "        print(f\"   ‚úÖ Features: {len(self.features_para_usar)} vari√°veis\")\n",
    "\n",
    "        return X_treino_scaled, y_treino, grid_treino_combinado\n",
    "\n",
    "    def processar_conjunto_teste(self, ano_teste):\n",
    "        \"\"\"Processa dados de teste usando APENAS informa√ß√µes do treino\"\"\"\n",
    "        print(f\"üìä Processando conjunto de TESTE ({ano_teste})...\")\n",
    "\n",
    "        grid_teste = self._processar_ano_individual(ano_teste, f\"Teste {ano_teste}\")\n",
    "        if grid_teste is None:\n",
    "            raise ValueError(f\"‚ùå N√£o foi poss√≠vel processar teste {ano_teste}\")\n",
    "\n",
    "        # Usar APENAS features definidas no treino\n",
    "        features_disponiveis = [f for f in self.features_para_usar if f in grid_teste.columns]\n",
    "\n",
    "        # Preencher features faltantes com 0\n",
    "        for feature in self.features_para_usar:\n",
    "            if feature not in grid_teste.columns:\n",
    "                print(f\"   ‚ö†Ô∏è  Feature {feature} n√£o encontrada no teste, preenchendo com 0\")\n",
    "                grid_teste[feature] = 0\n",
    "\n",
    "        X_teste = grid_teste[self.features_para_usar]\n",
    "\n",
    "        # Aplicar scaler treinado APENAS no treino\n",
    "        if self.scaler is None:\n",
    "            raise ValueError(\"‚ùå Scaler n√£o foi treinado no conjunto de treino!\")\n",
    "\n",
    "        X_teste_scaled = self.scaler.transform(X_teste)\n",
    "        y_teste = grid_teste['count']\n",
    "\n",
    "        print(f\"   ‚úÖ Teste: {len(grid_teste):,} c√©lulas\")\n",
    "\n",
    "        return X_teste_scaled, y_teste, grid_teste\n",
    "\n",
    "    def _definir_features(self, grid_treino):\n",
    "        \"\"\"Define features APENAS com base nos dados de treino\"\"\"\n",
    "        features_candidatas = ['x', 'y', 'periodos_diversidade', 'naturezas_diversidade',\n",
    "                               'locais_diversidade', 'idade_media', 'idade_std',\n",
    "                               'concentracao_temporal', 'densidade_vizinhanca']\n",
    "\n",
    "        # Manter apenas features presentes no treino\n",
    "        features_validas = [f for f in features_candidatas if f in grid_treino.columns]\n",
    "\n",
    "        # Verificar vari√¢ncia (remover features constantes)\n",
    "        features_finais = []\n",
    "        for feature in features_validas:\n",
    "            if grid_treino[feature].std() > 0.001:  # Threshold m√≠nimo de vari√¢ncia\n",
    "                features_finais.append(feature)\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  Removendo feature {feature} (vari√¢ncia muito baixa)\")\n",
    "\n",
    "        return features_finais\n",
    "\n",
    "    def _processar_ano_individual(self, ano, nome):\n",
    "        \"\"\"Processa um ano individualmente\"\"\"\n",
    "        print(f\"   üîß Processando {nome}...\")\n",
    "\n",
    "        try:\n",
    "            # Carregar dados\n",
    "            data_loader = DataLoader(self.config)\n",
    "            df_ano = data_loader.carregar_ano(ano)\n",
    "            if df_ano is None:\n",
    "                return None\n",
    "\n",
    "            # Processar\n",
    "            spatial_processor = SpatialProcessor(self.config)\n",
    "            gdf_ano = spatial_processor.processar_dados(df_ano, nome)\n",
    "            sao_paulo = spatial_processor.carregar_limites_sp()\n",
    "            grid_ano = spatial_processor.criar_grid_e_features(gdf_ano, sao_paulo, nome)\n",
    "\n",
    "            return grid_ano\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Erro ao processar {nome}: {e}\")\n",
    "            return None\n",
    "\n",
    "\n",
    "# ============ VISUALIZA√á√ÉO ============\n",
    "class RegressionVisualizator:\n",
    "    \"\"\"Gerencia visualiza√ß√µes para regress√£o\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def criar_mapa_regressao(self, grid_teste, y_real, y_pred, ano_teste, sao_paulo):\n",
    "        \"\"\"Cria mapa comparando valores reais vs previstos\"\"\"\n",
    "        print(f\"\\nüó∫Ô∏è Criando mapas de regress√£o - {ano_teste}\")\n",
    "\n",
    "        fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
    "\n",
    "        # Mapa 1: Valores Reais\n",
    "        sao_paulo.boundary.plot(ax=ax1, color='black', linewidth=0.5, alpha=0.5)\n",
    "        grid_real = grid_teste.copy()\n",
    "        grid_real['valor_real'] = y_real\n",
    "        grid_real['valor_real_log'] = np.log1p(grid_real['valor_real'])\n",
    "\n",
    "        grid_real.plot(ax=ax1, column='valor_real_log', cmap='Reds',\n",
    "                       legend=True, markersize=1,\n",
    "                       legend_kwds={'label': 'Log(Contagem Real + 1)'})\n",
    "\n",
    "        ax1.set_title(f'üìä VALORES REAIS - {ano_teste}', fontsize=12, weight='bold')\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "        # Mapa 2: Valores Preditos\n",
    "        sao_paulo.boundary.plot(ax=ax2, color='black', linewidth=0.5, alpha=0.5)\n",
    "        grid_pred = grid_teste.copy()\n",
    "        grid_pred['valor_predito'] = y_pred\n",
    "        grid_pred['valor_predito_log'] = np.log1p(grid_pred['valor_predito'])\n",
    "\n",
    "        grid_pred.plot(ax=ax2, column='valor_predito_log', cmap='Reds',\n",
    "                       legend=True, markersize=1,\n",
    "                       legend_kwds={'label': 'Log(Contagem Predita + 1)'})\n",
    "\n",
    "        ax2.set_title(f'ü§ñ VALORES PREDITOS - {ano_teste}', fontsize=12, weight='bold')\n",
    "        ax2.set_axis_off()\n",
    "\n",
    "        # Mapa 3: Erros (Res√≠duos)\n",
    "        sao_paulo.boundary.plot(ax=ax3, color='black', linewidth=0.5, alpha=0.5)\n",
    "        grid_erros = grid_teste.copy()\n",
    "        grid_erros['erro'] = y_real - y_pred\n",
    "        grid_erros['erro_abs'] = np.abs(grid_erros['erro'])\n",
    "\n",
    "        grid_erros.plot(ax=ax3, column='erro_abs', cmap='RdBu_r',\n",
    "                        legend=True, markersize=1,\n",
    "                        legend_kwds={'label': 'Erro Absoluto'})\n",
    "\n",
    "        mae = mean_absolute_error(y_real, y_pred)\n",
    "        ax3.set_title(f'üìà ERROS DA PREVIS√ÉO - {ano_teste}\\nMAE: {mae:.2f}',\n",
    "                      fontsize=12, weight='bold')\n",
    "        ax3.set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'mapa_regressao_{ano_teste}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def criar_grafico_dispersao(self, y_real, y_pred, ano_teste):\n",
    "        \"\"\"Cria gr√°fico de dispers√£o entre valores reais e previstos\"\"\"\n",
    "        plt.figure(figsize=(10, 8))\n",
    "\n",
    "        plt.scatter(y_real, y_pred, alpha=0.6, s=10)\n",
    "\n",
    "        max_val = max(y_real.max(), y_pred.max())\n",
    "        plt.plot([0, max_val], [0, max_val], 'r--', alpha=0.8, label='Previs√£o Perfeita')\n",
    "\n",
    "        plt.xlabel('Valores Reais')\n",
    "        plt.ylabel('Valores Preditos')\n",
    "        plt.title(f'Dispers√£o: Real vs Predito - {ano_teste}\\nR¬≤ = {r2_score(y_real, y_pred):.3f}')\n",
    "        plt.legend()\n",
    "        plt.grid(True, alpha=0.3)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'dispersao_regressao_{ano_teste}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "    def criar_mapa_hotspots_detalhado(self, grid_teste, y_real, y_pred, ano_teste, sao_paulo, threshold):\n",
    "        \"\"\"Cria mapa detalhado comparando hotspots reais vs previstos\"\"\"\n",
    "        print(f\"üó∫Ô∏è Criando mapa detalhado de hotspots - {ano_teste}\")\n",
    "\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "        # Mapa 1: Hotspots Reais\n",
    "        sao_paulo.boundary.plot(ax=ax1, color='black', linewidth=0.5, alpha=0.5)\n",
    "        grid_real = grid_teste.copy()\n",
    "        grid_real['hotspot_real'] = (y_real > threshold).astype(int)\n",
    "\n",
    "        # Plotar n√£o-hotspots e hotspots\n",
    "        nao_hotspots = grid_real[grid_real['hotspot_real'] == 0]\n",
    "        hotspots = grid_real[grid_real['hotspot_real'] == 1]\n",
    "\n",
    "        if not nao_hotspots.empty:\n",
    "            nao_hotspots.plot(ax=ax1, color='lightblue', alpha=0.6, markersize=1, label='N√£o Hotspot')\n",
    "        if not hotspots.empty:\n",
    "            hotspots.plot(ax=ax1, color='red', alpha=0.8, markersize=2, label='Hotspot Real')\n",
    "\n",
    "        ax1.set_title(\n",
    "            f'üî• HOTSPOTS REAIS - {ano_teste}\\nTotal: {hotspots.shape[0]:,} ({hotspots.shape[0] / len(grid_real) * 100:.1f}%)',\n",
    "            fontsize=12, weight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.set_axis_off()\n",
    "\n",
    "        # Mapa 2: Hotspots Previstos\n",
    "        sao_paulo.boundary.plot(ax=ax2, color='black', linewidth=0.5, alpha=0.5)\n",
    "        grid_pred = grid_teste.copy()\n",
    "        grid_pred['hotspot_predito'] = (y_pred > threshold).astype(int)\n",
    "\n",
    "        nao_hotspots_pred = grid_pred[grid_pred['hotspot_predito'] == 0]\n",
    "        hotspots_pred = grid_pred[grid_pred['hotspot_predito'] == 1]\n",
    "\n",
    "        if not nao_hotspots_pred.empty:\n",
    "            nao_hotspots_pred.plot(ax=ax2, color='lightblue', alpha=0.6, markersize=1, label='N√£o Hotspot (Previsto)')\n",
    "        if not hotspots_pred.empty:\n",
    "            hotspots_pred.plot(ax=ax2, color='red', alpha=0.8, markersize=2, label='Hotspot Previsto')\n",
    "\n",
    "        ax2.set_title(\n",
    "            f'ü§ñ HOTSPOTS PREVISTOS - {ano_teste}\\nTotal: {hotspots_pred.shape[0]:,} ({hotspots_pred.shape[0] / len(grid_pred) * 100:.1f}%)',\n",
    "            fontsize=12, weight='bold')\n",
    "        ax2.legend()\n",
    "        ax2.set_axis_off()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'mapa_hotspots_detalhado_{ano_teste}.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# ============ MODELO PREDITIVO CORRETO ============\n",
    "class CorrectPredictiveModel:\n",
    "    \"\"\"Modelo metodologicamente correto para predi√ß√£o temporal\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.model = None\n",
    "        self.processor = SafeTemporalProcessor(config)\n",
    "        self.visualizator = RegressionVisualizator(config)  # ‚úÖ VISUALIZA√á√ÉO INCLU√çDA\n",
    "\n",
    "    def executar_predicao_temporal_correta(self):\n",
    "        \"\"\"Executa predi√ß√£o temporal CORRETA: passado ‚Üí futuro\"\"\"\n",
    "        print(\"üöÄ INICIANDO PREDI√á√ÉO TEMPORAL CORRETA\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"üéØ CONFIGURA√á√ÉO:\")\n",
    "        print(f\"   ‚Ä¢ Treino: {self.config.ANOS_TREINO}\")\n",
    "        print(f\"   ‚Ä¢ Teste:  {self.config.ANO_TESTE}\")\n",
    "        print(f\"   ‚Ä¢ Garantia: SEM vazamento temporal\")\n",
    "        print(\"=\" * 60)\n",
    "\n",
    "        try:\n",
    "            # 1. Processar TREINO (apenas anos passados)\n",
    "            X_treino, y_treino, grid_treino = self.processor.processar_conjunto_treino(\n",
    "                self.config.ANOS_TREINO\n",
    "            )\n",
    "\n",
    "            # 2. Treinar modelo APENAS com dados de treino\n",
    "            print(\"üéØ Treinando modelo XGBoost...\")\n",
    "            self.model = xgb.XGBRegressor(**self.config.XGB_PARAMS)\n",
    "            self.model.fit(X_treino, y_treino)\n",
    "\n",
    "            # 3. Processar TESTE (usando apenas informa√ß√µes do treino)\n",
    "            X_teste, y_teste, grid_teste = self.processor.processar_conjunto_teste(\n",
    "                self.config.ANO_TESTE\n",
    "            )\n",
    "\n",
    "            # 4. Fazer previs√µes para o FUTURO\n",
    "            y_pred = self.model.predict(X_teste)\n",
    "\n",
    "            # 5. ‚úÖ GERAR VISUALIZA√á√ïES\n",
    "            print(\"üé® Gerando visualiza√ß√µes...\")\n",
    "            sao_paulo = SpatialProcessor(self.config).carregar_limites_sp()\n",
    "\n",
    "            # Criar mapas principais\n",
    "            self.visualizator.criar_mapa_regressao(grid_teste, y_teste, y_pred, self.config.ANO_TESTE, sao_paulo)\n",
    "            self.visualizator.criar_grafico_dispersao(y_teste, y_pred, self.config.ANO_TESTE)\n",
    "\n",
    "            # 6. Avalia√ß√£o rigorosa\n",
    "            resultado = self._avaliar_predicao_rigorosa(y_teste, y_pred, grid_teste)\n",
    "\n",
    "            # 7. Criar mapa de hotspots com threshold correto\n",
    "            threshold = resultado['hotspots']['threshold']\n",
    "            self.visualizator.criar_mapa_hotspots_detalhado(grid_teste, y_teste, y_pred, self.config.ANO_TESTE,\n",
    "                                                            sao_paulo, threshold)\n",
    "\n",
    "            # 8. An√°lise de import√¢ncia\n",
    "            self._analisar_importancia_features()\n",
    "\n",
    "            return resultado\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro na predi√ß√£o: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            return None\n",
    "\n",
    "    def _avaliar_predicao_rigorosa(self, y_teste, y_pred, grid_teste):\n",
    "        \"\"\"Avalia√ß√£o metodologicamente rigorosa\"\"\"\n",
    "        print(\"\\nüìä AVALIA√á√ÉO RIGOROSA DA PREDI√á√ÉO\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        # M√©tricas b√°sicas\n",
    "        mae = mean_absolute_error(y_teste, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_teste, y_pred))\n",
    "        r2 = r2_score(y_teste, y_pred)\n",
    "\n",
    "        print(f\"üìà M√©tricas de Regress√£o:\")\n",
    "        print(f\"   ‚Ä¢ R¬≤:   {r2:.4f}\")\n",
    "        print(f\"   ‚Ä¢ MAE:  {mae:.4f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE: {rmse:.4f}\")\n",
    "\n",
    "        # An√°lise de signific√¢ncia\n",
    "        self._analisar_significancia_r2(r2, len(y_teste))\n",
    "\n",
    "        # Converter para hotspots COM THRESHOLD CORRETO\n",
    "        resultado_hotspots = self._avaliar_hotspots(y_teste, y_pred)\n",
    "\n",
    "        # An√°lise de res√≠duos\n",
    "        residuos = y_teste - y_pred\n",
    "        print(f\"\\nüîç An√°lise de Res√≠duos:\")\n",
    "        print(f\"   ‚Ä¢ M√©dia: {residuos.mean():.4f}\")\n",
    "        print(f\"   ‚Ä¢ Std:   {residuos.std():.4f}\")\n",
    "        print(f\"   ‚Ä¢ RMSE/Std(y): {rmse / y_teste.std():.4f}\")\n",
    "\n",
    "        return {\n",
    "            'metricas_regressao': {'r2': r2, 'mae': mae, 'rmse': rmse},\n",
    "            'hotspots': resultado_hotspots,\n",
    "            'n_amostras': len(y_teste),\n",
    "            'y_real_stats': {\n",
    "                'mean': y_teste.mean(),\n",
    "                'std': y_teste.std(),\n",
    "                'min': y_teste.min(),\n",
    "                'max': y_teste.max()\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def _analisar_significancia_r2(self, r2, n_amostras):\n",
    "        \"\"\"Analisa se o R¬≤ √© estatisticamente significativo\"\"\"\n",
    "        print(f\"\\nüìä Signific√¢ncia Estat√≠stica (R¬≤ = {r2:.4f}):\")\n",
    "\n",
    "        if r2 > 0.7:\n",
    "            print(\"   ‚úÖ EXCELENTE - Modelo explica a maioria da vari√¢ncia\")\n",
    "        elif r2 > 0.5:\n",
    "            print(\"   üìä BOM - Modelo explica parte substancial da vari√¢ncia\")\n",
    "        elif r2 > 0.3:\n",
    "            print(\"   ‚ö†Ô∏è  MODERADO - Modelo tem algum poder explicativo\")\n",
    "        elif r2 > 0.1:\n",
    "            print(\"   üö® FRACO - Poder preditivo limitado\")\n",
    "        else:\n",
    "            print(\"   ‚ùå MUITO FRACO - Modelo praticamente n√£o explica a vari√¢ncia\")\n",
    "\n",
    "        # Regra pr√°tica: R¬≤ > 1/sqrt(n) para ser n√£o-trivial\n",
    "        limiar_nao_trivial = 1 / np.sqrt(n_amostras)\n",
    "        if r2 > limiar_nao_trivial:\n",
    "            print(f\"   ‚úÖ R¬≤ acima do limiar de n√£o-trivialidade ({limiar_nao_trivial:.4f})\")\n",
    "        else:\n",
    "            print(f\"   ‚ö†Ô∏è  R¬≤ abaixo do limiar de n√£o-trivialidade ({limiar_nao_trivial:.4f})\")\n",
    "\n",
    "    def _avaliar_hotspots(self, y_real, y_pred, percentil=85):\n",
    "        \"\"\"Avalia√ß√£o focada em hotspots COM THRESHOLD CORRETO\"\"\"\n",
    "        # Usar percentil dos dados REAIS para definir threshold\n",
    "        threshold = max(np.percentile(y_real, percentil), self.config.THRESHOLD_MINIMO)\n",
    "\n",
    "        y_real_bin = (y_real > threshold).astype(int)\n",
    "        y_pred_bin = (y_pred > threshold).astype(int)\n",
    "\n",
    "        accuracy = accuracy_score(y_real_bin, y_pred_bin)\n",
    "        precision = precision_score(y_real_bin, y_pred_bin, zero_division=0)\n",
    "        recall = recall_score(y_real_bin, y_pred_bin, zero_division=0)\n",
    "        f1 = f1_score(y_real_bin, y_pred_bin, zero_division=0)\n",
    "\n",
    "        print(f\"\\nüî• AVALIA√á√ÉO DE HOTSPOTS (threshold: {threshold:.2f} crimes):\")\n",
    "        print(f\"   ‚Ä¢ Hotspots reais: {y_real_bin.sum():,} ({y_real_bin.sum() / len(y_real) * 100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Hotspots previstos: {y_pred_bin.sum():,} ({y_pred_bin.sum() / len(y_pred) * 100:.1f}%)\")\n",
    "        print(f\"   ‚Ä¢ Acur√°cia:  {accuracy:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Precis√£o:  {precision:.4f}\")\n",
    "        print(f\"   ‚Ä¢ Recall:    {recall:.4f}\")\n",
    "        print(f\"   ‚Ä¢ F1-Score:  {f1:.4f}\")\n",
    "\n",
    "        # Diagn√≥stico adicional\n",
    "        if y_pred_bin.sum() > len(y_pred) * 0.5:\n",
    "            print(\"   ‚ö†Ô∏è  ALERTA: Mais de 50% das c√©lulas classificadas como hotspots!\")\n",
    "        if threshold == self.config.THRESHOLD_MINIMO:\n",
    "            print(\"   ‚ö†Ô∏è  ALERTA: Threshold atingiu o valor m√≠nimo\")\n",
    "\n",
    "        return {\n",
    "            'threshold': threshold,\n",
    "            'accuracy': accuracy,\n",
    "            'precision': precision,\n",
    "            'recall': recall,\n",
    "            'f1': f1,\n",
    "            'n_hotspots_real': y_real_bin.sum(),\n",
    "            'n_hotspots_pred': y_pred_bin.sum()\n",
    "        }\n",
    "\n",
    "    def _analisar_importancia_features(self):\n",
    "        \"\"\"An√°lise rigorosa da import√¢ncia das features\"\"\"\n",
    "        if self.model is None:\n",
    "            return\n",
    "\n",
    "        importancia = self.model.feature_importances_\n",
    "        features = self.processor.features_para_usar\n",
    "\n",
    "        df_importancia = pd.DataFrame({\n",
    "            'feature': features,\n",
    "            'importance': importancia\n",
    "        }).sort_values('importance', ascending=False)\n",
    "\n",
    "        print(f\"\\nüîç IMPORT√ÇNCIA DAS FEATURES:\")\n",
    "        for _, row in df_importancia.iterrows():\n",
    "            print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "        # An√°lise da feature mais importante\n",
    "        top_feature = df_importancia.iloc[0]\n",
    "        print(f\"   üéØ Feature mais importante: {top_feature['feature']} ({top_feature['importance']:.4f})\")\n",
    "\n",
    "\n",
    "# ============ VALIDA√á√ÉO CRUZADA TEMPORAL ============\n",
    "class CorrectTemporalValidator:\n",
    "    \"\"\"Valida√ß√£o cruzada temporal CORRETA\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "\n",
    "    def validar_abordagem_temporal(self):\n",
    "        \"\"\"Valida a abordagem temporal de forma correta\"\"\"\n",
    "        print(\"\\nüîÑ VALIDA√á√ÉO DA ABORDAGEM TEMPORAL\")\n",
    "        print(\"=\" * 50)\n",
    "\n",
    "        anos_ordenados = sorted(self.config.ANOS_DISPONIVEIS)\n",
    "        resultados = []\n",
    "\n",
    "        # Walk-forward validation\n",
    "        for i in range(1, len(anos_ordenados)):\n",
    "            anos_treino = anos_ordenados[:i]\n",
    "            ano_teste = anos_ordenados[i]\n",
    "\n",
    "            # Pular se ano_teste for o ano de teste principal\n",
    "            if ano_teste == self.config.ANO_TESTE:\n",
    "                continue\n",
    "\n",
    "            print(f\"   üîÅ {anos_treino} ‚Üí {ano_teste}\")\n",
    "\n",
    "            # Configura√ß√£o tempor√°ria\n",
    "            class TempConfig:\n",
    "                def __init__(self, anos_treino, ano_teste, anos_disponiveis, base_config):\n",
    "                    self.ANOS_TREINO = anos_treino\n",
    "                    self.ANO_TESTE = ano_teste\n",
    "                    self.ANOS_DISPONIVEIS = anos_disponiveis\n",
    "                    self.XGB_PARAMS = base_config.XGB_PARAMS\n",
    "                    self.DATA_PATH = base_config.DATA_PATH\n",
    "                    self.LAT_COL = base_config.LAT_COL\n",
    "                    self.LON_COL = base_config.LON_COL\n",
    "                    self.CELL_SIZE = base_config.CELL_SIZE\n",
    "                    self.HOTSPOT_PERCENTIL = base_config.HOTSPOT_PERCENTIL\n",
    "                    self.THRESHOLD_MINIMO = base_config.THRESHOLD_MINIMO\n",
    "\n",
    "            config_temp = TempConfig(anos_treino, ano_teste, anos_ordenados[:i + 1], self.config)\n",
    "\n",
    "            # Executar predi√ß√£o\n",
    "            model = CorrectPredictiveModel(config_temp)\n",
    "            resultado = model.executar_predicao_temporal_correta()\n",
    "\n",
    "            if resultado:\n",
    "                resultados.append({\n",
    "                    'anos_treino': anos_treino,\n",
    "                    'ano_teste': ano_teste,\n",
    "                    'r2': resultado['metricas_regressao']['r2']\n",
    "                })\n",
    "\n",
    "        # Resumo da valida√ß√£o\n",
    "        if resultados:\n",
    "            r2_values = [r['r2'] for r in resultados]\n",
    "            print(f\"\\nüìä RESUMO VALIDA√á√ÉO CRUZADA:\")\n",
    "            print(f\"   ‚Ä¢ R¬≤ m√©dio: {np.mean(r2_values):.4f}\")\n",
    "            print(f\"   ‚Ä¢ Std R¬≤:   {np.std(r2_values):.4f}\")\n",
    "            print(f\"   ‚Ä¢ Min R¬≤:   {np.min(r2_values):.4f}\")\n",
    "            print(f\"   ‚Ä¢ Max R¬≤:   {np.max(r2_values):.4f}\")\n",
    "\n",
    "        return resultados\n",
    "\n",
    "\n",
    "# ============ EXECU√á√ÉO PRINCIPAL ============\n",
    "def main_correta():\n",
    "    \"\"\"Execu√ß√£o metodologicamente correta\"\"\"\n",
    "    start_time = time.time()\n",
    "\n",
    "    config = Config()\n",
    "\n",
    "    print(\"üéØ PREDI√á√ÉO TEMPORAL - VERS√ÉO CORRIGIDA\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"GARANTIAS:\")\n",
    "    print(\"   ‚Ä¢ SEM vazamento temporal\")\n",
    "    print(\"   ‚Ä¢ Treino: apenas anos passados\")\n",
    "    print(\"   ‚Ä¢ Teste: apenas ano futuro\")\n",
    "    print(\"   ‚Ä¢ Processamento separado\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Predi√ß√£o principal\n",
    "    model = CorrectPredictiveModel(config)\n",
    "    resultado_principal = model.executar_predicao_temporal_correta()\n",
    "\n",
    "    # Valida√ß√£o temporal\n",
    "    validator = CorrectTemporalValidator(config)\n",
    "    resultados_validacao = validator.validar_abordagem_temporal()\n",
    "\n",
    "    tempo_total = time.time() - start_time\n",
    "    print(f\"\\n‚è±Ô∏è  Tempo total: {tempo_total / 60:.2f} minutos\")\n",
    "\n",
    "    if resultado_principal:\n",
    "        r2 = resultado_principal['metricas_regressao']['r2']\n",
    "        if r2 > 0.3:\n",
    "            print(\"‚úÖ PREDI√á√ÉO BEM-SUCEDIDA - Modelo tem poder preditivo\")\n",
    "        else:\n",
    "            print(\"‚ö†Ô∏è  PREDI√á√ÉO FRACA - Modelo tem poder preditivo limitado\")\n",
    "\n",
    "    return resultado_principal, resultados_validacao\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        import xgboost as xgb\n",
    "\n",
    "        print(f\"‚úÖ XGBoost dispon√≠vel - vers√£o {xgb.__version__}\")\n",
    "        resultado_principal, validacao = main_correta()\n",
    "    except ImportError:\n",
    "        print(\"‚ùå XGBoost n√£o instalado. Execute: pip install xgboost\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
